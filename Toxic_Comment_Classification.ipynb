{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYq45C4eB22ihqiYKsn3lf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003UJAN/Toxic-Comment-Classification/blob/main/Toxic_Comment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Packages**"
      ],
      "metadata": {
        "id": "HKOaUc1Uq6Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "g5fvYl5qPwuw",
        "outputId": "b40fe96f-a81c-43e5-f6e5-9aeb4e4a147c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from statistics import mean\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "import statistics\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "C46b4Gb2rUib"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few lines of the train.csv file\n",
        "!head -n 5 /content/train.csv\n",
        "\n",
        "# Print the last few lines of the train.csv file\n",
        "!tail -n 5 /content/train.csv"
      ],
      "metadata": {
        "id": "J4Q87zcuQXED",
        "outputId": "d2c320c0-8a3a-4ef1-ada1-60707865786c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n",
            "\"0000997932d777bf\",\"Explanation\n",
            "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",0,0,0,0,0,0\n",
            "\"000103f0d9cfb60f\",\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",0,0,0,0,0,0\n",
            "\"000113f07ec002fd\",\"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",0,0,0,0,0,0\n",
            " I am amazed to see that \"\"\"\"native speakers\"\"\"\" allegedly confirm the doughnut legend. OK, so JFK could have been understood as saying \"\"\"\"I'm a doughnut\"\"\"\". About as much as someone saying \"\"\"\"I'm a New Yorker\"\"\"\" could be understood as saying they are a magazine, a car, or a train.  \"\"\",0,0,0,0,0,0\n",
            "\"067f55436813a6be\",\"\"\"\n",
            "\n",
            "The quote is exclusive to the CBR piece though and, as I say goes to the \"\"\"\"did they jump or were they pushed?\"\"\"\" issue - the quote suggests they jumped. The Variety piece (a blog?) says they were fired but doesn't quote a source and that could just their spin. Again we should be careful about our wording as there seems to be different takes on this. For a fully balanced approach we'd need something directly from the production side of things. (  )\"\"\",0,0,0,0,0,0\n",
            "\"067f7b7f5e4ba20f\",\"Regarding edits made dur"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading & Pre-Processing**"
      ],
      "metadata": {
        "id": "EWLROUturbkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "test_y = pd.read_csv(\"/content/test_labels.csv\")"
      ],
      "metadata": {
        "id": "ko3OwfXosLY0",
        "outputId": "237b1804-42d2-41c1-8403-63fe1567ceee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 2430",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e43fffeb4161>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test_labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 2430"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "_MlqPhi6sTfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "f3yLdBt-sUWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "qhJ0rUDMsYEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y.head()"
      ],
      "metadata": {
        "id": "MuTieIK1sgpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "U6YPJJwSsjEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.shape"
      ],
      "metadata": {
        "id": "7XVZU0dcslfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(color_codes=True)\n",
        "comment_len = train.comment_text.str.len()\n",
        "sns.distplot(comment_len, kde=False, bins=20, color=\"steelblue\")"
      ],
      "metadata": {
        "id": "wFxxJ5lPsntv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsetting labels from the training data\n",
        "train_labels = train[['toxic', 'severe_toxic',\n",
        "                      'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "label_count = train_labels.sum()"
      ],
      "metadata": {
        "id": "pBHQ2E27stCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_count.plot(kind='bar', title='Labels Frequency', color='steelblue')"
      ],
      "metadata": {
        "id": "ziwmy_oNsvmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to draw bar graph for visualising distribution of classes within each label.\n",
        "barWidth = 0.25\n",
        "\n",
        "bars1 = [sum(train['toxic'] == 1), sum(train['obscene'] == 1), sum(train['insult'] == 1), sum(train['severe_toxic'] == 1),\n",
        "         sum(train['identity_hate'] == 1), sum(train['threat'] == 1)]\n",
        "bars2 = [sum(train['toxic'] == 0), sum(train['obscene'] == 0), sum(train['insult'] == 0), sum(train['severe_toxic'] == 0),\n",
        "         sum(train['identity_hate'] == 0), sum(train['threat'] == 0)]\n",
        "\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "\n",
        "plt.bar(r1, bars1, color='steelblue', width=barWidth, label='labeled = 1')\n",
        "plt.bar(r2, bars2, color='lightsteelblue', width=barWidth, label='labeled = 0')\n",
        "\n",
        "plt.xlabel('group', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate',\n",
        "                                                       'Threat'])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1hOuyAZMsyf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of clean comment\n",
        "train.comment_text[0]"
      ],
      "metadata": {
        "id": "_06p_J4qwMSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of toxic comment\n",
        "train[train.toxic == 1].iloc[1, 1]"
      ],
      "metadata": {
        "id": "Cj2YxqD_wOig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross correlation matrix across labels\n",
        "rowsums = train.iloc[:, 2:].sum(axis=1)\n",
        "temp = train.iloc[:, 2:-1]\n",
        "train_corr = temp[rowsums > 0]\n",
        "corr = train_corr.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr,\n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values, annot=True, cmap=\"Blues\""
      ],
      "metadata": {
        "id": "mKODhILpwRF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def W_Cloud(token):\n",
        "    \"\"\"\n",
        "    Visualize the most common words contributing to the token.\n",
        "    \"\"\"\n",
        "    threat_context = train[train[token] == 1]\n",
        "    threat_text = threat_context.comment_text\n",
        "    neg_text = pd.Series(threat_text).str.cat(sep=' ')\n",
        "    wordcloud = WordCloud(width=1600, height=800,\n",
        "                          max_font_size=200).generate(neg_text)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(wordcloud.recolor(colormap=\"Blues\"), interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Most common words assosiated with {token} comment\", size=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hTX5t0ECwUSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactive visual : enter the label name.\n",
        "token = input(\n",
        "    'Choose a class to visualize the most common words contributing to the class:')\n",
        "W_Cloud(token.lower())"
      ],
      "metadata": {
        "id": "1OBFkGTGwYWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**"
      ],
      "metadata": {
        "id": "aB37ecrTxDTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
        "               \"threat\", \"insult\", \"identity_hate\"]"
      ],
      "metadata": {
        "id": "U37McgNAxIGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    '''\n",
        "    Tokenize text and return a non-unique list of tokenized words found in the text.\n",
        "    Normalize to lowercase, strip punctuation, remove stop words, filter non-ascii characters.\n",
        "    Lemmatize the words and lastly drop words of length < 3.\n",
        "    '''\n",
        "    text = text.lower()\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
        "    nopunct = regex.sub(\" \", text)\n",
        "    words = nopunct.split(' ')\n",
        "    # remove any non ascii\n",
        "    words = [word.encode('ascii', 'ignore').decode('ascii') for word in words]\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    words = [lmtzr.lemmatize(w) for w in words]\n",
        "    words = [w for w in words if len(w) > 2]\n",
        "    return words"
      ],
      "metadata": {
        "id": "fWR1RtLRxKWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = TfidfVectorizer(ngram_range=(1, 1), analyzer='word',\n",
        "                         tokenizer=tokenize, stop_words='english',\n",
        "                         strip_accents='unicode', use_idf=1, min_df=10)\n",
        "X_train = vector.fit_transform(train['comment_text'])\n",
        "X_test = vector.transform(test['comment_text'])"
      ],
      "metadata": {
        "id": "Mtt5LfDqxPLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.get_feature_names()[0:20]"
      ],
      "metadata": {
        "id": "WFviZqO8xP1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "BaOGlbZxxU1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating classifiers with default parameters initially.\n",
        "clf1 = MultinomialNB()\n",
        "clf2 = LogisticRegression()\n",
        "clf3 = LinearSVC()"
      ],
      "metadata": {
        "id": "Sz3_npNfxY1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation_score(classifier, X_train, y_train):\n",
        "    '''\n",
        "    Iterate though each label and return the cross validation F1 and Recall score\n",
        "    '''\n",
        "    methods = []\n",
        "    name = classifier.__class__.__name__.split('.')[-1]\n",
        "\n",
        "    for label in test_labels:\n",
        "        recall = cross_val_score(\n",
        "            classifier, X_train, y_train[label], cv=10, scoring='recall')\n",
        "        f1 = cross_val_score(classifier, X_train,\n",
        "                             y_train[label], cv=10, scoring='f1')\n",
        "        methods.append([name, label, recall.mean(), f1.mean()])\n",
        "\n",
        "    return methods"
      ],
      "metadata": {
        "id": "ZCfJlzjNxde2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the cross validation F1 and Recall score for our 3 baseline models.\n",
        "methods1_cv = pd.DataFrame(cross_validation_score(clf1, X_train, train))\n",
        "methods2_cv = pd.DataFrame(cross_validation_score(clf2, X_train, train))\n",
        "methods3_cv = pd.DataFrame(cross_validation_score(clf3, X_train, train))"
      ],
      "metadata": {
        "id": "lP8wiz2z4A4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe to show summary of results.\n",
        "methods_cv = pd.concat([methods1_cv, methods2_cv, methods3_cv])\n",
        "methods_cv.columns = ['Model', 'Label', 'Recall', 'F1']\n",
        "meth_cv = methods_cv.reset_index()\n",
        "meth_cv[['Model', 'Label', 'Recall', 'F1']]"
      ],
      "metadata": {
        "id": "n04HCkE84aEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(classifier, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Calculate Hamming-loss, F1, Recall for each label on test dataset.\n",
        "    \"\"\"\n",
        "    methods = []\n",
        "    hloss = []\n",
        "    name = classifier.__class__.__name__.split('.')[-1]\n",
        "    predict_df = pd.DataFrame()\n",
        "    predict_df['id'] = test_y['id']\n",
        "\n",
        "    for label in test_labels:\n",
        "        classifier.fit(X_train, y_train[label])\n",
        "        predicted = classifier.predict(X_test)\n",
        "\n",
        "        predict_df[label] = predicted\n",
        "\n",
        "        recall = recall_score(y_test[y_test[label] != -1][label],\n",
        "                              predicted[y_test[label] != -1],\n",
        "                              average=\"weighted\")\n",
        "        f1 = f1_score(y_test[y_test[label] != -1][label],\n",
        "                      predicted[y_test[label] != -1],\n",
        "                      average=\"weighted\")\n",
        "\n",
        "        conf_mat = confusion_matrix(y_test[y_test[label] != -1][label],\n",
        "                                    predicted[y_test[label] != -1])\n",
        "\n",
        "        methods.append([name, label, recall, f1, conf_mat])\n",
        "\n",
        "    hamming_loss_score = hamming_loss(test_y[test_y['toxic'] != -1].iloc[:, 1:7],\n",
        "                                      predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
        "    hloss.append([name, hamming_loss_score])\n",
        "\n",
        "    return hloss, methods"
      ],
      "metadata": {
        "id": "JS3Pf36e4cX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Hamming-loss F1 and Recall score for our 3 baseline models.\n",
        "h1, methods1 = score(clf1, X_train, train, X_test, test_y)\n",
        "h2, methods2 = score(clf2, X_train, train, X_test, test_y)\n",
        "h3, methods3 = score(clf3, X_train, train, X_test, test_y)"
      ],
      "metadata": {
        "id": "fCB-z1Uf4kj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe to show summary of results.\n",
        "methods1 = pd.DataFrame(methods1)\n",
        "methods2 = pd.DataFrame(methods2)\n",
        "methods3 = pd.DataFrame(methods3)\n",
        "methods = pd.concat([methods1, methods2, methods3])\n",
        "methods.columns = ['Model', 'Label', 'Recall', 'F1', 'Confusion_Matrix']\n",
        "meth = methods.reset_index()\n",
        "meth[['Model', 'Label', 'Recall', 'F1']]"
      ],
      "metadata": {
        "id": "PZJIDEjM4nyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing F1 score results through box-plot.\n",
        "ax = sns.boxplot(x='Model', y='F1', data=methods, palette=\"Blues\")\n",
        "sns.stripplot(x='Model', y='F1', data=methods,\n",
        "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2, palette=\"Blues\")\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AoOolfjr6DWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualzing performance till now for each classifier across each category**"
      ],
      "metadata": {
        "id": "hkEH0QwQ6HHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create bar graph of F1 and Recall across each label for Multinomial Naive Bayes\n",
        "print(\"Plot for Multinomial Naive Bayes regression\")\n",
        "m2 = methods[methods.Model == 'MultinomialNB']\n",
        "\n",
        "m2.set_index([\"Label\"], inplace=True)\n",
        "%matplotlib inline\n",
        "m2.plot(figsize=(16, 8), kind='bar', title='Metrics',\n",
        "        rot=60, ylim=(0.0, 1), colormap='tab10')"
      ],
      "metadata": {
        "id": "a_zv1VIb6Jxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create bar graph of F1 and Recall across each label for Logistic regression\n",
        "print(\"Plot for Logistic regression\")\n",
        "m2 = methods[methods.Model == 'LogisticRegression']\n",
        "\n",
        "m2.set_index([\"Label\"], inplace=True)\n",
        "%matplotlib inline\n",
        "m2.plot(figsize=(16, 8), kind='bar', title='Metrics',\n",
        "        rot=60, ylim=(0.0, 1), colormap='tab10')"
      ],
      "metadata": {
        "id": "PUUz6mof6Maz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to create bar graph of F1 and Recall across each label for Linear SVC\n",
        "print(\"Plot for Linear SVC\")\n",
        "m2 = methods[methods.Model == 'LinearSVC']\n",
        "\n",
        "m2.set_index([\"Label\"], inplace=True)\n",
        "%matplotlib inline\n",
        "m2.plot(figsize=(16, 8), kind='bar', title='Metrics',\n",
        "        rot=60, ylim=(0.0, 1), colormap='tab10')"
      ],
      "metadata": {
        "id": "1autrTCy6QM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "D5L-WwxH6VBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drawConfusionMatrix(cm):\n",
        "    \"\"\"\n",
        "    Plot Confusion matrix of input cm.\n",
        "    \"\"\"\n",
        "    cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
        "    ax = plt.axes()\n",
        "    sns.heatmap(cm,\n",
        "                annot=True,\n",
        "                annot_kws={\"size\": 16},\n",
        "                cmap=\"Blues\",\n",
        "                fmt='.2f',\n",
        "                linewidths=2,\n",
        "                linecolor='steelblue',\n",
        "                xticklabels=(\"Non-toxic\", \"Toxic\"),\n",
        "                yticklabels=(\"Non-toxic\", \"Toxic\"))\n",
        "\n",
        "    plt.ylabel('True', fontsize=18)\n",
        "    plt.xlabel('Predicted', fontsize=18)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WWaWV92Z6YU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Matrix(label):\n",
        "    \"\"\"\n",
        "    Plot Confusion matrix for each label and call function drawConfusionMatrix().\n",
        "    \"\"\"\n",
        "    print(f\"*************** {label} labelling ***************\")\n",
        "    labels = {\"toxic\": 0, \"severe_toxic\": 1, \"obscene\": 2,\n",
        "              \"threat\": 3, \"insult\": 4, \"identity_hate\": 5}\n",
        "\n",
        "    pos = labels[label]\n",
        "    for i in range(pos, len(meth), 6):\n",
        "        print()\n",
        "        print(f\"****  {meth['Model'][i]}  ***\")\n",
        "        cm = meth['Confusion_Matrix'][i]\n",
        "        drawConfusionMatrix(cm)"
      ],
      "metadata": {
        "id": "u9bKuNMX6hwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = input('Choose a class for the Confusion Matrix: ')\n",
        "Matrix(token.lower())"
      ],
      "metadata": {
        "id": "oYnAytIR6lcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregated Hamming Loss Score**"
      ],
      "metadata": {
        "id": "DxFgTo2_6sWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe to summarize Hamming-loss\n",
        "hl1_df = pd.DataFrame(h1)\n",
        "hl2_df = pd.DataFrame(h2)\n",
        "hl3_df = pd.DataFrame(h3)"
      ],
      "metadata": {
        "id": "H73rkLqE6wiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hammingloss = pd.concat([hl1_df, hl2_df, hl3_df])\n",
        "hammingloss.columns = ['Model', 'Hamming_Loss']\n",
        "hl = hammingloss.reset_index()\n",
        "hl[['Model', 'Hamming_Loss']]"
      ],
      "metadata": {
        "id": "cUSuBmee6zJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipelines**"
      ],
      "metadata": {
        "id": "ts27xB0D66Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_lr = Pipeline([\n",
        "    ('lr', LogisticRegression(class_weight=\"balanced\"))\n",
        "])\n",
        "\n",
        "pipe_linear_svm = Pipeline([\n",
        "    ('svm', LinearSVC(class_weight={1: 20}))\n",
        "])\n",
        "\n",
        "pipelines = [pipe_lr, pipe_linear_svm]"
      ],
      "metadata": {
        "id": "pApgfBVf7AVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_df = []\n",
        "for pipe in pipelines:\n",
        "    f1_values = []\n",
        "    recall_values = []\n",
        "    hl = []\n",
        "    training_time = []\n",
        "    predict_df = pd.DataFrame()\n",
        "    predict_df['id'] = test_y['id']\n",
        "    for label in test_labels:\n",
        "        start = timer()\n",
        "        pipe.fit(X_train, train[label])\n",
        "        train_time = timer() - start\n",
        "        predicted = pipe.predict(X_test)\n",
        "        predict_df[label] = predicted\n",
        "\n",
        "        f1_values.append(f1_score(\n",
        "            test_y[test_y[label] != -1][label], predicted[test_y[label] != -1], average=\"weighted\"))\n",
        "        recall_values.append(recall_score(\n",
        "            test_y[test_y[label] != -1][label], predicted[test_y[label] != -1], average=\"weighted\"))\n",
        "        training_time.append(train_time)\n",
        "        name = pipe.steps[-1][1].__class__.__name__.split('.')[-1]\n",
        "\n",
        "    hamming_loss_score = hamming_loss(\n",
        "        test_y[test_y['toxic'] != -1].iloc[:, 1:7], predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
        "\n",
        "    val = [name, mean(f1_values), mean(recall_values),\n",
        "           hamming_loss_score, mean(training_time)]\n",
        "    score_df.append(val)"
      ],
      "metadata": {
        "id": "PV40KKTE7Cp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pd.DataFrame(score_df,)\n",
        "scores.columns = ['Model', 'F1', 'Recall', 'Hamming_Loss', 'Training_Time']\n",
        "scores"
      ],
      "metadata": {
        "id": "AOYfLyMl7FUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "mOxyq66D_Emd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Tuning"
      ],
      "metadata": {
        "id": "raJ4gjAR_Ruy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_classifier = LogisticRegression()\n",
        "\n",
        "parameter_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "                  'class_weight': [None, 'balanced']}\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=5)\n",
        "\n",
        "grid_search = GridSearchCV(logistic_regression_classifier,\n",
        "                           param_grid=parameter_grid,\n",
        "                           cv=cross_validation,\n",
        "                           scoring='f1')\n",
        "\n",
        "grid_search.fit(X_train, train['toxic'])\n",
        "\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "1aAso4jl_fWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Classifier Tuning"
      ],
      "metadata": {
        "id": "HjxlXgPk_m_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier = LinearSVC()\n",
        "\n",
        "parameter_grid = {'class_weight': [None, 'balanced'],\n",
        "                  'C': [1, 5, 10]}\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits=5)\n",
        "\n",
        "grid_search = GridSearchCV(svm_classifier,\n",
        "                           param_grid=parameter_grid,\n",
        "                           cv=cross_validation,\n",
        "                           scoring='f1')\n",
        "\n",
        "grid_search.fit(X_train, train['toxic'])\n",
        "\n",
        "print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "\n",
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "7T27Zmm4_uCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
        "                    intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "                    multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "                    verbose=0)\n",
        "\n",
        "lr_clf = lr_clf = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                                     intercept_scaling=1, max_iter=100, multi_class='warn',\n",
        "                                     n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
        "                                     tol=0.0001, verbose=0, warm_start=False)\n",
        "\n",
        "tunned_model_score_df = []\n",
        "for model in [svm_clf, lr_clf]:\n",
        "    f1_values = []\n",
        "    recall_values = []\n",
        "    hl = []\n",
        "    training_time = []\n",
        "    predict_df = pd.DataFrame()\n",
        "    predict_df['id'] = test_y['id']\n",
        "\n",
        "    for label in test_labels:\n",
        "        start = timer()\n",
        "        model.fit(X_train, train[label])\n",
        "        training_time.append(timer() - start)\n",
        "        predicted = model.predict(X_test)\n",
        "        predict_df[label] = predicted\n",
        "\n",
        "        f1_values.append(f1_score(test_y[test_y[label] != -1][label],\n",
        "                                  predicted[test_y[label] != -1],\n",
        "                                  average=\"weighted\"))\n",
        "        recall_values.append(recall_score(test_y[test_y[label] != -1][label],\n",
        "                                          predicted[test_y[label] != -1],\n",
        "                                          average=\"weighted\"))\n",
        "        name = model.__class__.__name__\n",
        "\n",
        "    hamming_loss_score = hamming_loss(test_y[test_y['toxic'] != -1].iloc[:, 1:7],\n",
        "                                      predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
        "\n",
        "    val = [name, mean(f1_values), mean(recall_values),\n",
        "           hamming_loss_score, sum(training_time)]\n",
        "\n",
        "    tunned_model_score_df.append(val)"
      ],
      "metadata": {
        "id": "eDTNpysi_0Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunned_scores = pd.DataFrame(tunned_model_score_df,)\n",
        "tunned_scores.columns = ['Model', 'F1',\n",
        "                         'Recall', 'Hamming_Loss', 'Traing_Time']\n",
        "tunned_scores"
      ],
      "metadata": {
        "id": "7nGRFF2-_08v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensembling**"
      ],
      "metadata": {
        "id": "dqmCxrgAHqJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ab_clf = AdaBoostClassifier()\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "boosting_models = [ab_clf, gb_clf, xgb_clf]"
      ],
      "metadata": {
        "id": "or1soyJKHubt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boosting_score_df = []\n",
        "for model in boosting_models:\n",
        "    f1_values = []\n",
        "    recall_values = []\n",
        "    training_time = []\n",
        "    hloss = []\n",
        "    predict_df = pd.DataFrame()\n",
        "    predict_df['id'] = test_y['id']\n",
        "\n",
        "    for idx, label in enumerate(test_labels):\n",
        "        start = timer()\n",
        "        model.fit(X_train, train[label])\n",
        "        predicted = model.predict(X_test)\n",
        "        training_time.append(timer() - start)\n",
        "        predict_df[label] = predicted\n",
        "        f1_values.append(f1_score(test_y[test_y[label] != -1][label],\n",
        "                                  predicted[test_y[label] != -1],\n",
        "                                  average=\"weighted\"))\n",
        "        recall_values.append(recall_score(test_y[test_y[label] != -1][label],\n",
        "                                          predicted[test_y[label] != -1],\n",
        "                                          average=\"weighted\"))\n",
        "        name = model.__class__.__name__\n",
        "\n",
        "    hamming_loss_score = hamming_loss(test_y[test_y['toxic'] != -1].iloc[:, 1:7],\n",
        "                                      predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
        "\n",
        "    val = [name, mean(f1_values), mean(recall_values),\n",
        "           hamming_loss_score, mean(training_time)]\n",
        "\n",
        "    boosting_score_df.append(val)"
      ],
      "metadata": {
        "id": "Pks0XW8oHwnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boosting_score = pd.DataFrame(boosting_score_df,)\n",
        "boosting_score.columns = ['Model', 'F1',\n",
        "                          'Recall', 'Hamming_Loss', 'Traing_Time']\n",
        "boosting_score"
      ],
      "metadata": {
        "id": "fqKAKh_fHzDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_clf = VotingClassifier(estimators=[('lr', lr_clf),\n",
        "                                            ('svm', svm_clf),\n",
        "                                            ('xgb', xgb_clf)], voting='hard')\n",
        "ensemble_score_df = []\n",
        "f1_values = []\n",
        "recall_values = []\n",
        "hl = []\n",
        "training_time = []\n",
        "\n",
        "predict_df = pd.DataFrame()\n",
        "predict_df['id'] = test_y['id']\n",
        "for label in test_labels:\n",
        "    start = timer()\n",
        "    ensemble_clf.fit(X_train, train[label])\n",
        "    training_time.append(timer() - start)\n",
        "    predicted = ensemble_clf.predict(X_test)\n",
        "    predict_df[label] = predicted\n",
        "    f1_values.append(f1_score(test_y[test_y[label] != -1][label],\n",
        "                              predicted[test_y[label] != -1],\n",
        "                              average=\"weighted\"))\n",
        "    recall_values.append(recall_score(test_y[test_y[label] != -1][label],\n",
        "                                      predicted[test_y[label] != -1],\n",
        "                                      average=\"weighted\"))\n",
        "    name = 'Ensemble'\n",
        "\n",
        "hamming_loss_score = hamming_loss(test_y[test_y['toxic'] != -1].iloc[:, 1:7],\n",
        "                                  predict_df[test_y['toxic'] != -1].iloc[:, 1:7])\n",
        "\n",
        "val = [name, mean(f1_values), mean(recall_values),\n",
        "       hamming_loss_score, mean(training_time)]\n",
        "ensemble_score_df.append(val)\n",
        "\n",
        "\n",
        "# printing the values\n",
        "ensemble_score = pd.DataFrame(ensemble_score_df,)\n",
        "ensemble_score.columns = ['Model', 'F1',\n",
        "                          'Recall', 'Hamming_Loss', 'Training_Time']\n",
        "ensemble_score"
      ],
      "metadata": {
        "id": "lYElntVnH7Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'toxic'\n",
        "lr = LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
        "               intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "               multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "               verbose=0)\n",
        "lr.fit(X_train, train[label])\n",
        "Toxic_LR = lr.predict(X_test)\n",
        "test_combined = pd.concat([test, test_y], axis=1)"
      ],
      "metadata": {
        "id": "iYImobNOH-CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract wrongly classified comments\n",
        "commentCheck = test_combined[(test_combined.toxic == 1) & (\n",
        "    Toxic_LR == 0)].comment_text\n",
        "\n",
        "neg_Check = pd.Series(commentCheck).str.cat(sep=' ')\n",
        "wordcloud = WordCloud(width=1600, height=800,\n",
        "                      max_font_size=200).generate(neg_Check)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(wordcloud.recolor(colormap=\"Blues\"), interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most common words from misclassified\", size=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ZwkIvKaH-rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrongWords = tokenize(neg_Check)\n",
        "stop_words = stopwords.words('English')\n",
        "wrongWords = [w for w in wrongWords if w not in stop_words]\n",
        "cntr = Counter(wrongWords)\n",
        "cntr.most_common(20)"
      ],
      "metadata": {
        "id": "R9jxPaO6IJLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neg_text_train = train['comment_text'].str.cat(sep=' ')\n",
        "cntr_train = Counter(tokenize(neg_text_train))\n",
        "cntr_train.get('ucking')"
      ],
      "metadata": {
        "id": "wpO5BHdeIMCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**"
      ],
      "metadata": {
        "id": "Ftj0iZGSISn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Plot learning rate curve for the estimator with title, training data as X,\n",
        "    labels as y.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator,\n",
        "                                                            X, y, train_sizes=train_sizes, cv=cv, n_jobs=n_jobs)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"steelblue\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"olive\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "metadata": {
        "id": "qSVkdyceIVOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Learning Curves (Linear SVC) for TOXIC\"\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
        "estimator = LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
        "                      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "                      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "                      verbose=0)\n",
        "plot_learning_curve(estimator, title, X_train,\n",
        "                    train['toxic'], ylim=(0.7, 1.01), cv=cv, n_jobs=4)"
      ],
      "metadata": {
        "id": "0jv-3BCWIXkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}